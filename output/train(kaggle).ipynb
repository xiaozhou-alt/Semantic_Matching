{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12529270,"sourceType":"datasetVersion","datasetId":7909184},{"sourceId":480662,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":385583,"modelId":404785}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\n\n# 配置参数\nMAX_LEN = 128  # 最大序列长度\nEMBED_DIM = 256  # 增加嵌入维度\nPROJECTION_DIM = 512  # 增加投影维度\nBATCH_SIZE = 512  # 批大小\nEPOCHS = 50  # 训练轮数\nVOCAB_SIZE = 33958  # 词汇表大小\n\n# 创建保存结果的目录\nos.makedirs('/kaggle/working/training_results', exist_ok=True)\n\n# 1. 数据加载与预处理\ndef load_data(file_path):\n    data = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) < 3:\n                continue\n            sent1, sent2, label = parts[0], parts[1], int(parts[2])\n            data.append({\n                'sent1_ids': [int(x) for x in sent1.split()],\n                'sent2_ids': [int(x) for x in sent2.split()],\n                'label': label\n            })\n    return pd.DataFrame(data)\n\n# 加载数据\ndf = load_data('/kaggle/input/semantic-matching/train.tsv')\n\n# 2. 构建词汇表（使用实际数据中的最大ID）\nvocab_size = max(\n    max(df['sent1_ids'].max()), \n    max(df['sent2_ids'].max())\n) + 1\nprint(f\"实际词汇表大小: {vocab_size}\")\n\n# 3. 数据预处理\ndef preprocess_data(df):\n    # 填充序列\n    df['sent1_padded'] = df['sent1_ids'].apply(\n        lambda x: x[:MAX_LEN] + [0] * (MAX_LEN - len(x)))\n    df['sent2_padded'] = df['sent2_ids'].apply(\n        lambda x: x[:MAX_LEN] + [0] * (MAX_LEN - len(x)))\n    \n    # 转换为numpy数组\n    X1 = np.array(df['sent1_padded'].tolist())\n    X2 = np.array(df['sent2_padded'].tolist())\n    y = np.array(df['label'])\n    \n    return X1, X2, y\n\nX1, X2, y = preprocess_data(df)\n\n# 4. 划分训练集和验证集 (80%训练, 20%验证)\nX1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n    X1, X2, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 5. 构建改进的双塔模型\ndef create_encoder():\n    input_layer = layers.Input(shape=(MAX_LEN,))\n    \n    # 词嵌入层\n    embedding = layers.Embedding(\n        input_dim=vocab_size, \n        output_dim=EMBED_DIM,\n        mask_zero=True,\n        embeddings_regularizer=l2(1e-5)  # 添加正则化\n    )(input_layer)\n    \n    # 位置编码\n    position_embedding = layers.Embedding(\n        input_dim=MAX_LEN,\n        output_dim=EMBED_DIM\n    )(tf.range(start=0, limit=MAX_LEN, delta=1))\n    embedding += position_embedding\n    \n    # 双向LSTM + 注意力机制\n    lstm = layers.Bidirectional(layers.LSTM(\n        128, \n        return_sequences=True,\n        kernel_regularizer=l2(1e-5),\n        recurrent_regularizer=l2(1e-5)\n    ))(embedding)\n    \n    # 自注意力机制\n    attention = layers.MultiHeadAttention(\n        num_heads=4, \n        key_dim=64,\n        value_dim=64\n    )(lstm, lstm)\n    \n    # 残差连接 + 层归一化\n    lstm = layers.Add()([lstm, attention])\n    lstm = layers.LayerNormalization()(lstm)\n    \n    # 卷积层\n    conv1 = layers.Conv1D(256, 3, activation='relu', padding='same')(lstm)\n    conv2 = layers.Conv1D(256, 5, activation='relu', padding='same')(conv1)\n    \n    # 全局平均池化 + 最大池化\n    avg_pool = layers.GlobalAveragePooling1D()(conv2)\n    max_pool = layers.GlobalMaxPooling1D()(conv2)\n    pooled = layers.Concatenate()([avg_pool, max_pool])\n    \n    # 投影层\n    projection = layers.Dense(PROJECTION_DIM, activation='relu')(pooled)\n    projection = layers.Dropout(0.3)(projection)  # 增加dropout\n    \n    return models.Model(inputs=input_layer, outputs=projection)\n\n# 创建双塔模型\nsent1_input = layers.Input(shape=(MAX_LEN,), dtype='int32')\nsent2_input = layers.Input(shape=(MAX_LEN,), dtype='int32')\n\nencoder = create_encoder()\nsent1_encoded = encoder(sent1_input)\nsent2_encoded = encoder(sent2_input)\n\n# 特征融合和相似度计算\n# 1. 绝对差\ndiff = layers.Subtract()([sent1_encoded, sent2_encoded])\nabs_diff = layers.Lambda(lambda x: tf.abs(x))(diff)\n\n# 2. 点积相似度\ncosine_sim = layers.Dot(axes=1, normalize=True)([sent1_encoded, sent2_encoded])\n\n# 3. 拼接特征\nmerged = layers.Concatenate()([sent1_encoded, sent2_encoded, abs_diff])\n\n# 多层感知机\ndense1 = layers.Dense(256, activation='relu')(merged)\ndense1 = layers.Dropout(0.3)(dense1)\ndense2 = layers.Dense(128, activation='relu')(dense1)\ndense2 = layers.Dropout(0.3)(dense2)\n\n# 输出层\noutput = layers.Dense(1, activation='sigmoid')(dense2)\n\nmodel = models.Model(inputs=[sent1_input, sent2_input], outputs=output)\n\n# 6. 编译模型（修复学习率设置问题）\ninitial_learning_rate = 1e-3\noptimizer = Adam(learning_rate=initial_learning_rate)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=[tf.keras.metrics.AUC(name='auc')]\n)\n\n# 7. 回调函数\n# 模型检查点\nmodel_checkpoint = callbacks.ModelCheckpoint(\n    filepath='/kaggle/working/semantic_matching_model_best.h5',\n    monitor='val_auc',\n    mode='max',\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1\n)\n\n# 早停\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_auc', \n    patience=10,  # 增加耐心值\n    mode='max', \n    restore_best_weights=True,\n    verbose=1\n)\n\n# 学习率调度器 - 使用指数衰减\nlr_scheduler = callbacks.LearningRateScheduler(\n    lambda epoch, lr: lr * 0.9 if epoch > 5 else lr\n)\n\n# 8. 训练模型\nprint(\"开始训练模型...\")\nhistory = model.fit(\n    [X1_train, X2_train], \n    y_train,\n    validation_data=([X1_val, X2_val], y_val),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopping, model_checkpoint, lr_scheduler],\n    verbose=1\n)\nprint(\"模型训练完成!\")\n\n# 9. 保存最终模型\nmodel.save('/kaggle/working/semantic_matching_model_final.h5')\n\n# 10. 可视化训练历史\ndef plot_training_history(history):\n    # 创建图表\n    plt.figure(figsize=(15, 6))\n    \n    # AUC图表\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['auc'], label='training_AUC')\n    plt.plot(history.history['val_auc'], label='val_AUC')\n    plt.title('training&val_AUC')\n    plt.ylabel('AUC')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # 损失图表\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='training_loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.title('training and val loss')\n    plt.ylabel('loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # 保存图表\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/training_results/training_history.png')\n    plt.close()\n    \n    # 单独保存最佳AUC图表\n    plt.figure(figsize=(10, 6))\n    plt.plot(history.history['val_auc'], 'o-', label='验证 AUC')\n    plt.title('val_AUC-Epoch')\n    plt.ylabel('AUC')\n    plt.xlabel('Epoch')\n    \n    # 标记最佳AUC点\n    best_epoch = np.argmax(history.history['val_auc'])\n    best_auc = history.history['val_auc'][best_epoch]\n    plt.plot(best_epoch, best_auc, 'ro', markersize=10, label=f'best_AUC: {best_auc:.4f}')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('/kaggle/working/training_results/validation_auc_history.png')\n    plt.close()\n    \n    return best_auc\n\n# 绘制训练历史\nbest_auc = plot_training_history(history)\n\n# 11. 验证集评估\nval_preds = model.predict([X1_val, X2_val]).flatten()\nval_auc = roc_auc_score(y_val, val_preds)\nprint(f\"\\n最终模型验证集AUC: {val_auc:.4f}\")\nprint(f\"训练过程中的最佳验证AUC: {best_auc:.4f}\")\n\n# 12. 保存预测结果示例\nval_results = pd.DataFrame({\n    '真实标签': y_val,\n    '预测概率': val_preds\n})\nval_results.to_csv('/kaggle/working/training_results/validation_predictions.csv', index=False)\n\nprint(\"所有结果已保存到 /kaggle/working/training_results 目录\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T07:47:56.677436Z","iopub.execute_input":"2025-07-21T07:47:56.677981Z","iopub.status.idle":"2025-07-21T09:03:28.366192Z","shell.execute_reply.started":"2025-07-21T07:47:56.677958Z","shell.execute_reply":"2025-07-21T09:03:28.365406Z"}},"outputs":[{"name":"stderr","text":"2025-07-21 07:47:58.478897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753084078.672929      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753084078.729715      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"实际词汇表大小: 33958\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753084104.757868      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"开始训练模型...\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753084118.032791      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.8134 - loss: 0.5238\nEpoch 1: val_auc improved from -inf to 0.88775, saving model to /kaggle/working/semantic_matching_model_best.h5\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 189ms/step - auc: 0.8134 - loss: 0.5237 - val_auc: 0.8877 - val_loss: 0.5309 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9148 - loss: 0.3901\nEpoch 2: val_auc improved from 0.88775 to 0.90609, saving model to /kaggle/working/semantic_matching_model_best.h5\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 187ms/step - auc: 0.9148 - loss: 0.3901 - val_auc: 0.9061 - val_loss: 0.5105 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9397 - loss: 0.3431\nEpoch 3: val_auc improved from 0.90609 to 0.92683, saving model to /kaggle/working/semantic_matching_model_best.h5\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 187ms/step - auc: 0.9397 - loss: 0.3431 - val_auc: 0.9268 - val_loss: 0.5019 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9536 - loss: 0.3111\nEpoch 4: val_auc did not improve from 0.92683\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9536 - loss: 0.3111 - val_auc: 0.9262 - val_loss: 0.5099 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9625 - loss: 0.2877\nEpoch 5: val_auc improved from 0.92683 to 0.93700, saving model to /kaggle/working/semantic_matching_model_best.h5\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9625 - loss: 0.2877 - val_auc: 0.9370 - val_loss: 0.5417 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9686 - loss: 0.2691\nEpoch 6: val_auc did not improve from 0.93700\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 186ms/step - auc: 0.9686 - loss: 0.2692 - val_auc: 0.9353 - val_loss: 0.5657 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9749 - loss: 0.2467\nEpoch 7: val_auc did not improve from 0.93700\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9749 - loss: 0.2467 - val_auc: 0.9301 - val_loss: 0.6188 - learning_rate: 9.0000e-04\nEpoch 8/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9805 - loss: 0.2216\nEpoch 8: val_auc did not improve from 0.93700\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9805 - loss: 0.2216 - val_auc: 0.9368 - val_loss: 0.6111 - learning_rate: 8.1000e-04\nEpoch 9/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9848 - loss: 0.1985\nEpoch 9: val_auc improved from 0.93700 to 0.93929, saving model to /kaggle/working/semantic_matching_model_best.h5\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 187ms/step - auc: 0.9848 - loss: 0.1985 - val_auc: 0.9393 - val_loss: 0.5605 - learning_rate: 7.2900e-04\nEpoch 10/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9883 - loss: 0.1776\nEpoch 10: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9883 - loss: 0.1776 - val_auc: 0.9373 - val_loss: 0.5555 - learning_rate: 6.5610e-04\nEpoch 11/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9912 - loss: 0.1568\nEpoch 11: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9912 - loss: 0.1568 - val_auc: 0.9336 - val_loss: 0.7047 - learning_rate: 5.9049e-04\nEpoch 12/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9934 - loss: 0.1395\nEpoch 12: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9934 - loss: 0.1395 - val_auc: 0.9378 - val_loss: 0.6770 - learning_rate: 5.3144e-04\nEpoch 13/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9950 - loss: 0.1235\nEpoch 13: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9950 - loss: 0.1235 - val_auc: 0.9351 - val_loss: 0.6973 - learning_rate: 4.7830e-04\nEpoch 14/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9963 - loss: 0.1105\nEpoch 14: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9963 - loss: 0.1105 - val_auc: 0.9325 - val_loss: 0.7523 - learning_rate: 4.3047e-04\nEpoch 15/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc: 0.9973 - loss: 0.0968\nEpoch 15: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 186ms/step - auc: 0.9973 - loss: 0.0968 - val_auc: 0.9335 - val_loss: 0.7875 - learning_rate: 3.8742e-04\nEpoch 16/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.9980 - loss: 0.0870\nEpoch 16: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 187ms/step - auc: 0.9980 - loss: 0.0870 - val_auc: 0.9296 - val_loss: 0.8695 - learning_rate: 3.4868e-04\nEpoch 17/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.9984 - loss: 0.0787\nEpoch 17: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 187ms/step - auc: 0.9984 - loss: 0.0787 - val_auc: 0.9281 - val_loss: 0.8987 - learning_rate: 3.1381e-04\nEpoch 18/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.9988 - loss: 0.0706\nEpoch 18: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 187ms/step - auc: 0.9988 - loss: 0.0706 - val_auc: 0.9241 - val_loss: 1.0393 - learning_rate: 2.8243e-04\nEpoch 19/50\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc: 0.9990 - loss: 0.0655\nEpoch 19: val_auc did not improve from 0.93929\n\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 187ms/step - auc: 0.9990 - loss: 0.0655 - val_auc: 0.9234 - val_loss: 1.1101 - learning_rate: 2.5419e-04\nEpoch 19: early stopping\nRestoring model weights from the end of the best epoch: 9.\n模型训练完成!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/735364216.py:248: UserWarning: Glyph 39564 (\\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.\n  plt.savefig('/kaggle/working/training_results/validation_auc_history.png')\n/tmp/ipykernel_36/735364216.py:248: UserWarning: Glyph 35777 (\\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.\n  plt.savefig('/kaggle/working/training_results/validation_auc_history.png')\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step\n\n最终模型验证集AUC: 0.9412\n训练过程中的最佳验证AUC: 0.9393\n所有结果已保存到 /kaggle/working/training_results 目录\n","output_type":"stream"}],"execution_count":1}]}